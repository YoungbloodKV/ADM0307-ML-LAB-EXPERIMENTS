{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54afbdba",
   "metadata": {},
   "source": [
    "EXP-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e79ac719",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rule 1: IF Outlook = Rainy THEN Yes\n",
      "Rule 2: IF Temperature = Cool THEN Yes\n",
      "Rule 3: IF Temperature = Mild THEN Yes\n"
     ]
    }
   ],
   "source": [
    "# Simple dataset: (Outlook, Temp, Label)\n",
    "data = [\n",
    "    (\"Sunny\", \"Hot\", \"No\"),\n",
    "    (\"Sunny\", \"Mild\", \"Yes\"),\n",
    "    (\"Rainy\", \"Cool\", \"Yes\"),\n",
    "    (\"Rainy\", \"Mild\", \"Yes\"),\n",
    "    (\"Sunny\", \"Cool\", \"Yes\")\n",
    "]\n",
    "\n",
    "# Split positives and negatives\n",
    "positives = [x for x in data if x[2] == \"Yes\"]\n",
    "negatives = [x for x in data if x[2] == \"No\"]\n",
    "\n",
    "rules = []\n",
    "\n",
    "def learn_rule(positives, negatives):\n",
    "    # Try to find simple rule: Outlook=value or Temp=value\n",
    "    attributes = [0, 1]  # index of attributes\n",
    "    for attr in attributes:\n",
    "        values = set([p[attr] for p in positives])\n",
    "        for v in values:\n",
    "            # Rule: IF attr=v THEN Yes\n",
    "            covered_pos = [p for p in positives if p[attr] == v]\n",
    "            covered_neg = [n for n in negatives if n[attr] == v]\n",
    "\n",
    "            # Sequential covering: rule must not cover negatives\n",
    "            if len(covered_neg) == 0:\n",
    "                return (attr, v), covered_pos\n",
    "    return None, []\n",
    "\n",
    "# Main Sequential Covering Loop\n",
    "while positives:\n",
    "    rule, covered = learn_rule(positives, negatives)\n",
    "    if rule is None:\n",
    "        break\n",
    "    rules.append(rule)\n",
    "    for c in covered:\n",
    "        positives.remove(c)\n",
    "\n",
    "# Print learned rules\n",
    "for i, r in enumerate(rules, 1):\n",
    "    attr = \"Outlook\" if r[0] == 0 else \"Temperature\"\n",
    "    print(f\"Rule {i}: IF {attr} = {r[1]} THEN Yes\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baebae0d",
   "metadata": {},
   "source": [
    "EXP-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cbd9424a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IF petal length (cm) <= 2.45 THEN class = setosa\n",
      "IF petal length (cm) > 2.45 AND petal width (cm) <= 1.75 AND petal length (cm) <= 4.95 THEN class = versicolor\n",
      "IF petal length (cm) > 2.45 AND petal width (cm) <= 1.75 AND petal length (cm) > 4.95 THEN class = virginica\n",
      "IF petal length (cm) > 2.45 AND petal width (cm) > 1.75 AND petal length (cm) <= 4.85 THEN class = virginica\n",
      "IF petal length (cm) > 2.45 AND petal width (cm) > 1.75 AND petal length (cm) > 4.85 THEN class = virginica\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier, _tree\n",
    "import numpy as np\n",
    "\n",
    "# Load dataset\n",
    "X, y = load_iris(return_X_y=True)\n",
    "clf = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
    "clf.fit(X, y)\n",
    "\n",
    "feature_names = load_iris().feature_names\n",
    "target_names = load_iris().target_names\n",
    "\n",
    "rules = []\n",
    "\n",
    "# Recursive rule extraction\n",
    "def extract_rules(node, rule):\n",
    "    if tree.feature[node] != _tree.TREE_UNDEFINED:\n",
    "        name = feature_names[tree.feature[node]]\n",
    "        threshold = tree.threshold[node]\n",
    "\n",
    "        # left child rule\n",
    "        extract_rules(tree.children_left[node],\n",
    "                      rule + [f\"{name} <= {threshold:.2f}\"])\n",
    "\n",
    "        # right child rule\n",
    "        extract_rules(tree.children_right[node],\n",
    "                      rule + [f\"{name} > {threshold:.2f}\"])\n",
    "    else:\n",
    "        # Leaf → convert to rule\n",
    "        class_label = target_names[np.argmax(tree.value[node])]\n",
    "        rules.append((rule, class_label))\n",
    "\n",
    "tree = clf.tree_\n",
    "extract_rules(0, [])\n",
    "\n",
    "# Print rules\n",
    "for r, label in rules:\n",
    "    print(\"IF \" + \" AND \".join(r) + f\" THEN class = {label}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a07f04",
   "metadata": {},
   "source": [
    "EXP-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4476b520",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explanation for positive example:\n",
      "- blockA is light\n",
      "- blockB is stable\n",
      "\n",
      "Generalized Rule:\n",
      "IF light(X) AND stable(Y) THEN can_stack(X, Y)\n"
     ]
    }
   ],
   "source": [
    "# --- Domain Theory (facts + rules) ---\n",
    "domain = {\n",
    "    \"light\": [\"blockA\", \"blockC\"],\n",
    "    \"stable\": [\"blockB\", \"blockD\"]\n",
    "}\n",
    "\n",
    "# Rule: can_stack(X,Y) if light(X) and stable(Y)\n",
    "def can_stack(x, y):\n",
    "    return (x in domain[\"light\"]) and (y in domain[\"stable\"])\n",
    "\n",
    "# Positive Example\n",
    "example = (\"blockA\", \"blockB\")\n",
    "\n",
    "# EBL Explanation + Generalization\n",
    "def explain_and_generalize(example):\n",
    "    x, y = example\n",
    "    \n",
    "    # Explain why the example is positive\n",
    "    if can_stack(x, y):\n",
    "        explanation = [\n",
    "            f\"{x} is light\",\n",
    "            f\"{y} is stable\"\n",
    "        ]\n",
    "        \n",
    "        # Generalized EBL Rule\n",
    "        generalized_rule = \"IF light(X) AND stable(Y) THEN can_stack(X, Y)\"\n",
    "        \n",
    "        return explanation, generalized_rule\n",
    "    else:\n",
    "        return None, None\n",
    "\n",
    "exp, rule = explain_and_generalize(example)\n",
    "\n",
    "print(\"Explanation for positive example:\")\n",
    "for e in exp:\n",
    "    print(\"-\", e)\n",
    "\n",
    "print(\"\\nGeneralized Rule:\")\n",
    "print(rule)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e03220",
   "metadata": {},
   "source": [
    "EXP-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd20c98c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Q-Table:\n",
      " [[ 3.03753622  4.58      ]\n",
      " [ 3.08185241  6.2       ]\n",
      " [ 3.79695929  8.        ]\n",
      " [ 5.90604401 10.        ]\n",
      " [ 0.          0.        ]]\n",
      "\n",
      "Optimal Policy: ['right', 'right', 'right', 'right', 'left']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Environment states (0 to 4)\n",
    "states = [0, 1, 2, 3, 4]\n",
    "actions = [\"left\", \"right\"]\n",
    "\n",
    "# Q-table (5 states × 2 actions)\n",
    "Q = np.zeros((5, 2))\n",
    "\n",
    "alpha = 0.5     # learning rate\n",
    "gamma = 0.9     # discount factor\n",
    "epsilon = 0.2   # exploration rate\n",
    "\n",
    "def step(state, action):\n",
    "    if action == \"left\":\n",
    "        next_state = max(0, state - 1)\n",
    "    else:  # \"right\"\n",
    "        next_state = min(4, state + 1)\n",
    "\n",
    "    reward = 10 if next_state == 4 else -1\n",
    "    return next_state, reward\n",
    "\n",
    "# Training episodes\n",
    "for episode in range(50):\n",
    "    state = 0  # start always at state 0\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        # ε-greedy action selection\n",
    "        if random.random() < epsilon:\n",
    "            a = random.choice([0, 1])\n",
    "        else:\n",
    "            a = np.argmax(Q[state])\n",
    "\n",
    "        action = actions[a]\n",
    "        next_state, reward = step(state, action)\n",
    "\n",
    "        # Q-learning update rule\n",
    "        Q[state, a] = Q[state, a] + alpha * (\n",
    "            reward + gamma * np.max(Q[next_state]) - Q[state, a]\n",
    "        )\n",
    "\n",
    "        state = next_state\n",
    "        if state == 4:\n",
    "            done = True\n",
    "\n",
    "print(\"Final Q-Table:\\n\", Q)\n",
    "\n",
    "# Extract optimal policy\n",
    "policy = [actions[np.argmax(Q[s])] for s in states]\n",
    "print(\"\\nOptimal Policy:\", policy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c675e7",
   "metadata": {},
   "source": [
    "EXP-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e2658f2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Reward (Random Policy): -10.9\n",
      "\n",
      "Learned Optimal Policy (state 0–8):\n",
      "['down', 'down', 'down', 'right', 'right', 'down', 'right', 'right', 'up']\n",
      "\n",
      "Q-Table:\n",
      " [[ 3.0834321   4.58        2.94059752  4.27284022]\n",
      " [-0.975       6.19941958 -1.30125    -1.2125    ]\n",
      " [ 2.12031226  7.45312473 -1.25625    -0.975     ]\n",
      " [ 2.5584458   1.759375    4.4400146   6.2       ]\n",
      " [ 4.51860817  7.046875    4.53854366  8.        ]\n",
      " [ 3.82656178 10.          4.65        6.9296875 ]\n",
      " [-0.5        -0.5         0.          6.234375  ]\n",
      " [-0.5         0.          0.          9.9609375 ]\n",
      " [ 0.          0.          0.          0.        ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Grid World 3x3 → states 0–8\n",
    "states = list(range(9))\n",
    "\n",
    "# Actions\n",
    "actions = [\"up\",\"down\",\"left\",\"right\"]\n",
    "\n",
    "# Q-table (9 states × 4 actions)\n",
    "Q = np.zeros((9, 4))\n",
    "\n",
    "alpha = 0.5\n",
    "gamma = 0.9\n",
    "epsilon = 0.2\n",
    "\n",
    "def to_state(r, c): return r * 3 + c\n",
    "def to_pos(s): return divmod(s, 3)\n",
    "\n",
    "# Movement rules\n",
    "def step(state, action):\n",
    "    r, c = to_pos(state)\n",
    "\n",
    "    if action == \"up\":    r = max(0, r - 1)\n",
    "    if action == \"down\":  r = min(2, r + 1)\n",
    "    if action == \"left\":  c = max(0, c - 1)\n",
    "    if action == \"right\": c = min(2, c + 1)\n",
    "\n",
    "    next_state = to_state(r, c)\n",
    "    reward = 10 if next_state == 8 else -1   # Goal at state 8\n",
    "    done = (next_state == 8)\n",
    "    return next_state, reward, done\n",
    "\n",
    "# -------- Random Policy Baseline --------\n",
    "def random_policy_episodes(n=20):\n",
    "    total = 0\n",
    "    for _ in range(n):\n",
    "        s = 0\n",
    "        episode_reward = 0\n",
    "        done = False\n",
    "        while not done:\n",
    "            a = random.choice(actions)\n",
    "            s, r, done = step(s, a)\n",
    "            episode_reward += r\n",
    "        total += episode_reward\n",
    "    return total / n\n",
    "\n",
    "baseline_reward = random_policy_episodes()\n",
    "\n",
    "# -------- Q-Learning Training --------\n",
    "for ep in range(100):\n",
    "    s = 0\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        # epsilon-greedy\n",
    "        if random.random() < epsilon:\n",
    "            a_idx = random.randint(0,3)\n",
    "        else:\n",
    "            a_idx = np.argmax(Q[s])\n",
    "\n",
    "        next_s, reward, done = step(s, actions[a_idx])\n",
    "\n",
    "        # TD Update\n",
    "        Q[s, a_idx] = Q[s, a_idx] + alpha * (\n",
    "            reward + gamma * np.max(Q[next_s]) - Q[s, a_idx]\n",
    "        )\n",
    "\n",
    "        s = next_s\n",
    "\n",
    "# Optimal policy after learning\n",
    "policy = [actions[np.argmax(Q[s])] for s in states]\n",
    "\n",
    "print(\"Baseline Reward (Random Policy):\", baseline_reward)\n",
    "print(\"\\nLearned Optimal Policy (state 0–8):\")\n",
    "print(policy)\n",
    "print(\"\\nQ-Table:\\n\", Q)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
